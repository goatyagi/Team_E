{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "dc336987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "            home  there  up  PerfectV  NonPerfectV  Num_Verbs Author\n",
      "text01.txt     0      0   0         0            1          1      A\n",
      "text02.txt     0      0   0         0            1          1      A\n",
      "text03.txt     0      0   0         0            1          1      A\n",
      "text04.txt     0      0   0         0            1          1      A\n",
      "text05.txt     0      0   0         0            1          1      A\n",
      "text06.txt     0      0   0         0            1          1      A\n",
      "text07.txt     0      0   0         0            1          1      A\n",
      "text08.txt     0      0   0         0            0          0      A\n",
      "text09.txt     1      0   0         0            1          1      A\n",
      "text10.txt     0      0   0         0            1          1      A\n",
      "text11.txt     0      0   0         1            0          1      B\n",
      "text12.txt     0      0   0         1            0          1      B\n",
      "text13.txt     0      0   0         0            1          1      B\n",
      "text14.txt     0      0   0         0            1          1      B\n",
      "text15.txt     0      0   0         0            1          1      B\n",
      "text16.txt     0      0   0         0            1          1      B\n",
      "text17.txt     0      0   0         0            1          1      B\n",
      "text18.txt     0      0   0         0            1          1      B\n",
      "text19.txt     0      0   0         0            1          1      B\n",
      "text20.txt     0      0   0         0            1          1      B\n",
      "text21.txt     1      0   0         1            1          2      C\n",
      "text22.txt     0      0   0         1            1          2      C\n",
      "text23.txt     0      1   0         1            1          2      C\n",
      "text24.txt     0      0   0         1            1          2      C\n",
      "text25.txt     1      0   0         1            1          2      C\n",
      "text26.txt     0      0   0         1            0          1      C\n",
      "text27.txt     0      0   0         1            1          2      C\n",
      "text28.txt     0      0   0         1            1          2      C\n",
      "text29.txt     0      1   1         1            1          2      C\n",
      "text30.txt     0      0   0         1            1          2      C\n",
      "text31.txt     0      0   0         1            0          1      D\n",
      "text32.txt     0      0   0         1            0          1      D\n",
      "text33.txt     0      0   0         1            0          1      D\n",
      "text34.txt     0      0   0         1            0          1      D\n",
      "text35.txt     0      0   0         1            0          1      D\n",
      "text36.txt     0      0   0         1            0          1      D\n",
      "text37.txt     0      0   0         1            0          1      D\n",
      "text38.txt     0      0   0         1            1          2      D\n",
      "text39.txt     0      0   0         2            0          2      D\n",
      "text40.txt     0      0   0         2            0          2      D\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/gotoyagi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/gotoyagi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "import os\n",
    "\n",
    "files = [ i for i in os.listdir(\"./Data/TextData/\") if re.match('.*(.txt)$', i)]\n",
    "files.sort()\n",
    "\n",
    "document = []\n",
    "\n",
    "for f in files:\n",
    "    docs = open(\"./Data/TextData/\"+f, \"r\").read()\n",
    "    document.append(docs)\n",
    "    \n",
    "words = []\n",
    "Pos = []\n",
    "\n",
    "for i in range(len(document)):\n",
    "    words.append(nltk.word_tokenize(document[i]))\n",
    "    Pos.append(nltk.pos_tag(words[i]))\n",
    "    \n",
    "place = ['abroad', 'above', 'below', 'downstairs', 'upstairs', 'far', 'here', 'there', 'home', 'near', 'nowhere', 'anywhere', 'everywhere', 'outside', 'inside', 'under', 'up', 'across', 'around', 'away', 'beside', 'beyond', ]\n",
    "time = ['now', 'today', 'tomorrow', 'yesterday', 'recently', 'lately', 'soon', 'early', 'already', 'then', 'still', 'yet', 'later', 'immediately', 'finally', 'before','after', 'afterwards']\n",
    "\n",
    "\n",
    "P = []\n",
    "T = []\n",
    "\n",
    "for i in range(len(document)):\n",
    "    place_ad =[]\n",
    "    time_ad = []\n",
    "    \n",
    "    for factor in words[i]:\n",
    "        if factor.lower() in place:\n",
    "            place_ad.append(factor.lower())\n",
    "        elif factor.lower() in time:\n",
    "            time_ad.append(factor.lower())\n",
    "                    \n",
    "    P.append(len(place_ad))\n",
    "    T.append(len(time_ad))\n",
    "\n",
    "print(P, T)\n",
    "\n",
    "#---------------------------------------\n",
    "\n",
    "PerfectV = []\n",
    "NonPerfectV = []\n",
    "\n",
    "for i in range(len(document)):    \n",
    "    \n",
    "    v_p = []\n",
    "    v_np = []\n",
    "    flag = 0 \n",
    "    \n",
    "    for j in range (len(Pos[i])):\n",
    "        if flag != 0:\n",
    "            flag -= 1\n",
    "            continue\n",
    "            \n",
    "        if re.match(\"have|has|had\", Pos[i][j][0].lower()):\n",
    "            if Pos[i][j+1][1] == 'VBN' or Pos[i][j+1][0] == 'had':\n",
    "                v_p.append(Pos[i][j+1][0])\n",
    "                if Pos[i][j+1][0] == 'had':\n",
    "                    flag = 1\n",
    "            elif Pos[i][j+2][1] == 'VBN' or Pos[i][j+2][0] == 'had':\n",
    "                v_p.append(Pos[i][j+2][0])\n",
    "                if Pos[i][j+2][0] == 'had':\n",
    "                    flag = 2\n",
    "            else:\n",
    "                v_np.append(Pos[i][j][0])\n",
    "        elif Pos[i][j][1].startswith('VB') and Pos[i][j][1] != 'VBN' and Pos[i][j][1] != 'VBG':\n",
    "            v_np.append(Pos[i][j][0])\n",
    "    \n",
    "    PerfectV.append(len(v_p))\n",
    "    NonPerfectV.append(len(v_np))\n",
    "    \n",
    "author = pd.read_csv(\"./Data/Authordata/author.csv\", index_col = 0)\n",
    "Au = []\n",
    "\n",
    "for i in range(len(author)):\n",
    "    Au.append(author['Author'][i])\n",
    "    \n",
    "table[\"PerfectV\"] = PerfectV\n",
    "table[\"NonPerfectV\"] = NonPerfectV\n",
    "table[\"Num_Verbs\"] = table[\"PerfectV\"] + table[\"NonPerfectV\"]\n",
    "table[\"Author\"] = Au\n",
    "\n",
    "print(table)\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
